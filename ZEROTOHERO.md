# üöÄ ALS Caregiver's Compass - Zero to Hero Guide

**Complete End-to-End Documentation**  
*From Installation to Production Deployment*

---

## üìö Table of Contents

1. [Project Overview](#project-overview)
2. [System Architecture](#system-architecture)
3. [Tech Stack](#tech-stack)
4. [Data Flow & Processing](#data-flow--processing)
5. [Setup from Scratch](#setup-from-scratch)
6. [Directory Structure](#directory-structure)
7. [Running the Application](#running-the-application)
8. [Feature Deep Dive](#feature-deep-dive)
9. [API Reference](#api-reference)
10. [Troubleshooting](#troubleshooting)
11. [Future Improvements](#future-improvements)

---

## üéØ Project Overview

### What is ALS Caregiver's Compass?

A **RAG (Retrieval-Augmented Generation)** powered web application designed to provide **ALS caregivers in India** with:
- ü§ñ AI-powered Q&A assistance
- üìö Curated medical knowledge
- üí¨ Community experiences from WhatsApp groups
- üè• Emergency protocols & care guides
- üîÑ Multi-model AI flexibility (OpenAI, Gemini, Claude, Grok)

### Key Features

1. **Multi-Model AI System**
   - Runtime switching between 4 LLM providers
   - Model selection dropdown in UI
   - Automatic fallback on errors

2. **RAG Implementation**
   - Vector database (ChromaDB)
   - Semantic search
   - Source prioritization (India-focused)

3. **Comprehensive Information**
   - Emergency protocols
   - Daily care schedules
   - Communication aids
   - Home ICU setup guides

4. **Safe Database Management**
   - Clear warnings before deletion
   - Optimized data ingestion
   - Progress tracking

---

## üèóÔ∏è System Architecture

### High-Level Architecture

```mermaid
graph TB
    subgraph "Frontend Layer"
        A[Web Browser] --> B[HTML/CSS/JS]
        B --> C[AI Assistant UI]
        C --> D[Model Selector]
    end
    
    subgraph "Backend Layer"
        E[Flask App] --> F[API Routes]
        F --> G[UnifiedAISystem]
        G --> H{Model Router}
        H --> I[OpenAI]
        H --> J[Gemini]
        H --> K[Claude]
        H --> L[Grok]
    end
    
    subgraph "Data Layer"
        M[VectorStore] --> N[ChromaDB]
        N --> O[Medical Knowledge]
        N --> P[Community Experiences]
        M --> Q[Sentence Transformers]
    end
    
    subgraph "Data Sources"
        R[sources.yaml] --> S[Data Ingestion]
        T[WhatsApp Chats] --> S
        S --> M
    end
    
    B --> E
    G --> M
    
    style A fill:#e1f5ff
    style E fill:#fff3e0
    style M fill:#f3e5f5
    style S fill:#e8f5e9
```

### Component Breakdown

#### 1. **Frontend (Presentation Layer)**
```
templates/
‚îú‚îÄ‚îÄ base.html          ‚Üí Master template
‚îú‚îÄ‚îÄ ai_assistant.html  ‚Üí Chat interface + model selector
‚îú‚îÄ‚îÄ index.html         ‚Üí Landing page
‚îú‚îÄ‚îÄ emergency_protocol.html
‚îî‚îÄ‚îÄ ... (other pages)

static/
‚îú‚îÄ‚îÄ css/style.css     ‚Üí Design system
‚îî‚îÄ‚îÄ js/main.js        ‚Üí Client-side logic
```

#### 2. **Backend (Application Layer)**
```python
app.py                 # Flask application
‚îú‚îÄ‚îÄ Routes
‚îÇ   ‚îú‚îÄ‚îÄ / (home)
‚îÇ   ‚îú‚îÄ‚îÄ /ai-assistant
‚îÇ   ‚îî‚îÄ‚îÄ /api/ai-assistant (POST)
‚îî‚îÄ‚îÄ Session Management
```

#### 3. **AI System (Intelligence Layer)**
```python
ai_system_unified.py
‚îú‚îÄ‚îÄ UnifiedAISystem
‚îÇ   ‚îú‚îÄ‚îÄ _init_openai()
‚îÇ   ‚îú‚îÄ‚îÄ _init_gemini()
‚îÇ   ‚îú‚îÄ‚îÄ _init_claude()
‚îÇ   ‚îú‚îÄ‚îÄ _init_grok()
‚îÇ   ‚îú‚îÄ‚îÄ process_query()
‚îÇ   ‚îú‚îÄ‚îÄ _retrieve_context()
‚îÇ   ‚îú‚îÄ‚îÄ _synthesize_response()
‚îÇ   ‚îî‚îÄ‚îÄ _detect_emergency()
```

#### 4. **Data Layer (Storage & Retrieval)**
```python
vector_store.py
‚îú‚îÄ‚îÄ VectorStore
‚îÇ   ‚îú‚îÄ‚îÄ __init__()
‚îÇ   ‚îú‚îÄ‚îÄ add_to_collection()
‚îÇ   ‚îú‚îÄ‚îÄ query()
‚îÇ   ‚îú‚îÄ‚îÄ get_collection_count()
‚îÇ   ‚îî‚îÄ‚îÄ clear_collection()
```

---

## üíª Tech Stack

### Frontend Technologies

| Technology | Version | Purpose |
|------------|---------|---------|
| **HTML5** | - | Structure & semantic markup |
| **CSS3** | - | Styling, design system |
| **JavaScript (ES6+)** | - | Client-side interactivity |
| **Font Awesome** | 6.x | Icons |
| **Mermaid.js** | Latest | Flowchart rendering |

### Backend Technologies

| Technology | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.11+ | Core language |
| **Flask** | 2.3+ | Web framework |
| **python-dotenv** | Latest | Environment management |

### AI & ML Stack

| Technology | Purpose | When Used |
|------------|---------|-----------|
| **OpenAI API** | GPT-4o-mini LLM | AI responses (default) |
| **Google Gemini API** | Gemini 2.0 Flash LLM | AI responses (free tier) |
| **Anthropic API** | Claude Sonnet 4.5 LLM | AI responses (best quality) |
| **xAI Grok API** | Grok 2 LLM | AI responses (alternative) |
| **Sentence Transformers** | all-MiniLM-L6-v2 | Text embeddings (local) |
| **ChromaDB** | 0.4.18 | Vector database |

### Data Processing

| Library | Purpose |
|---------|---------|
| **PyYAML** | Parse medical sources |
| **NumPy** | Vector operations |
| **tqdm** | Progress bars |

---

## üîÑ Data Flow & Processing

### 1. Data Ingestion Flow

```mermaid
sequenceDiagram
    participant User
    participant Script as ingest_data_fast.py
    participant VS as VectorStore
    participant ST as SentenceTransformer
    participant DB as ChromaDB

    User->>Script: python ingest_data_fast.py
    Script->>Script: Load sources.yaml
    Script->>Script: Load whatsapp_anonymized.txt
    
    loop For each source
        Script->>Script: Scrub PII
        Script->>ST: Generate embedding
        ST-->>Script: Vector [384 dims]
        Script->>VS: add_to_collection()
        VS->>DB: Store text + vector
    end
    
    Script->>User: ‚úÖ Loaded 12 medical + 448 community
```

**Key Steps:**
1. **Load data files** (YAML + TXT)
2. **Scrub PII** (remove emails, phones)
3. **Generate embeddings** (384-dimensional vectors)
4. **Store in ChromaDB** (2 collections)
   - `medical_knowledge` - Trust score: 8-10
   - `community_experiences` - Trust score: 7

### 2. Query Processing Flow

```mermaid
sequenceDiagram
    participant User
    participant UI as Browser
    participant Flask
    participant AI as UnifiedAISystem
    participant VS as VectorStore
    participant LLM as Selected LLM
    participant DB as ChromaDB

    User->>UI: Select model (e.g., OpenAI)
    User->>UI: Type question
    UI->>Flask: POST /api/ai-assistant
    
    Flask->>AI: process_query(question, model='openai')
    
    AI->>AI: detect_emergency()
    
    AI->>VS: query(question)
    VS->>DB: Semantic search
    DB-->>VS: Top 5 documents
    VS-->>AI: Ranked results
    
    AI->>AI: prepare_context()
    Note over AI: Prioritize India sources
    
    AI->>LLM: Generate response
    LLM-->>AI: AI-generated answer
    
    AI-->>Flask: Response + metadata
    Flask-->>UI: JSON response
    UI->>User: Display answer
```

**Key Steps:**
1. **User selects model** from dropdown
2. **Types question** in chat interface
3. **Emergency detection** (keywords check)
4. **Semantic search** in ChromaDB
   - Convert query to vector
   - Find similar documents
   - Return top 5 results
5. **Context preparation**
   - Score documents (India sources = +2)
   - Format for LLM
6. **LLM synthesis**
   - Send context + question
   - Get structured response
7. **Return to user** with sources

### 3. Model Selection Flow

```mermaid
graph LR
    A[User Selects Model] --> B{Which Model?}
    
    B -->|openai| C[OpenAI Client]
    B -->|gemini| D[Gemini Client]
    B -->|claude| E[Claude Client]
    B -->|grok| F[Grok Client]
    
    C --> G[API Call]
    D --> G
    E --> G
    F --> G
    
    G --> H{Success?}
    H -->|Yes| I[Return Response]
    H -->|No| J[Fallback Message]
    
    style A fill:#e1f5ff
    style G fill:#fff3e0
    style I fill:#e8f5e9
    style J fill:#ffebee
```

---

## üöÄ Setup from Scratch

### Prerequisites

Before starting, ensure you have:

‚úÖ **Python 3.11+** installed  
‚úÖ **pip** package manager  
‚úÖ **Git** (optional, for cloning)  
‚úÖ **Text editor** (VS Code recommended)  
‚úÖ **At least ONE API key** (OpenAI or Gemini recommended)

---

### Step 1: Project Setup

#### Option A: Clone Repository (if available)
```bash
git clone <repository-url>
cd als-caregiver-compass
```

#### Option B: Verify Existing Project
```bash
cd als-caregiver-compass
ls  # Should show app.py, requirements.txt, etc.
```

---

### Step 2: Create Virtual Environment

**Windows:**
```powershell
python -m venv venv
.\venv\Scripts\activate
```

**macOS/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

You should see `(venv)` in your terminal prompt.

---

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**What gets installed:**

| Package | Purpose |
|---------|---------|
| flask | Web framework |
| openai | OpenAI API client |
| anthropic | Claude API client |
| google-generativeai | Gemini API client |
| chromadb | Vector database |
| sentence-transformers | Embeddings |
| python-dotenv | Environment variables |
| PyYAML | YAML parsing |
| tqdm | Progress bars |
| numpy | Vector math |

**Expected output:**
```
Successfully installed flask-X.X.X openai-X.X.X ...
```

---

### Step 4: Get API Keys

#### Option 1: OpenAI (Recommended for Production)

1. Visit: https://platform.openai.com/api-keys
2. Click **"+ Create new secret key"**
3. Name it (e.g., "ALS Compass")
4. Copy key: `sk-proj-xxxxxxxxxxxxx`
5. Save securely

**Cost:** ~$0.15 per 1M tokens (~$0.50 per 1000 questions)

#### Option 2: Google Gemini (Free Tier)

1. Visit: https://aistudio.google.com/app/apikey
2. Click **"Create API Key"**
3. Copy key: `AIzaxxxxxxxxxxxxxx`
4. Free tier: 15 requests/min, 1500/day

**Cost:** FREE (with limits)

#### Option 3: Anthropic Claude (Best Quality)

1. Visit: https://console.anthropic.com/settings/keys
2. Click **"Create Key"**
3. Add payment method
4. Buy credits ($5 minimum)
5. Copy key: `sk-ant-xxxxxxxxxxxxx`

**Cost:** ~$3 per 1M tokens (~$10 per 1000 questions)

#### Option 4: xAI Grok (Alternative)

1. Visit: https://console.x.ai/
2. Create account
3. Generate API key
4. Copy key

---

### Step 5: Configure Environment Variables

Create `.env` file in project root:

```bash
# In project root
touch .env  # macOS/Linux
# OR just create file in editor on Windows
```

**Add this content:**

```env
# ========== GENERAL SETTINGS ==========
SECRET_KEY=your-random-secret-key-change-me-in-production
FLASK_ENV=development

# ========== AI MODEL API KEYS ==========
# Add at least ONE of these:

# OpenAI (RECOMMENDED)
OPENAI_API_KEY=sk-proj-your-actual-key-here
OPENAI_MODEL=gpt-4o-mini

# Google Gemini (FREE)
GEMINI_API_KEY=AIza-your-actual-key-here
GEMINI_MODEL=gemini-2.0-flash-exp

# Anthropic Claude (BEST QUALITY)
ANTHROPIC_API_KEY=sk-ant-your-actual-key-here
CLAUDE_MODEL=claude-sonnet-4-20250514

# xAI Grok (OPTIONAL)
XAI_API_KEY=xai-your-actual-key-here
GROK_MODEL=grok-2-latest

# ========== DEFAULT MODEL ==========
# Which model to use by default (openai, gemini, claude, grok)
DEFAULT_MODEL_PROVIDER=openai

# ========== DATABASE ==========
CHROMA_DB_PATH=./chroma_db
```

**Important:**
- Replace `your-actual-key-here` with real keys
- Keep at least ONE provider configured
- `DEFAULT_MODEL_PROVIDER` sets initial selection

---

### Step 6: Prepare Data Files

Your `data/` folder should contain:

```
data/
‚îú‚îÄ‚îÄ sources.yaml               # Medical sources metadata
‚îî‚îÄ‚îÄ whatsapp_anonymized.txt    # Community chats
```

**Verify files exist:**
```bash
ls data/
# Should show: sources.yaml  whatsapp_anonymized.txt
```

**If missing:**
- Get from project repository
- Or create sample files

---

### Step 7: Ingest Data

Run the optimized ingestion script:

```bash
python ingest_data_fast.py
```

**What happens:**

1. **Checks data files** exist
2. **Shows current database** status
3. **Asks confirmation** if data exists
4. **Loads medical sources** (~12 items)
5. **Loads WhatsApp messages** (max 500)
6. **Shows progress bars**
7. **Completes in ~30 seconds**

**Expected output:**
```
======================================================================
  üì• ALS CAREGIVER'S COMPASS - DATA INGESTION
======================================================================

üìÅ Checking data files...
   ‚úÖ data/sources.yaml
   ‚úÖ data/whatsapp_anonymized.txt

üîß Initializing database...
   ‚úÖ Vector store ready

======================================================================
Starting data ingestion...
======================================================================

üìö Loading medical sources...
‚úÖ Loaded 12 medical sources

üí¨ Loading WhatsApp (max 500 messages)...
   Found 134484 total lines in file
   Processing 448 valid messages
Processing messages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 448/448 [00:39<00:00, 11.24it/s]
‚úÖ Loaded 448 WhatsApp messages

======================================================================
üìä INGESTION COMPLETE!
======================================================================

‚úÖ Medical sources:     12 loaded
‚úÖ Community messages:  448 loaded

======================================================================

üöÄ Next step:
   Run: python app.py
   Then visit: http://127.0.0.1:5000

======================================================================
```

**If prompted to overwrite:**
- Type `OVERWRITE` to confirm
- Type anything else to cancel

---

### Step 8: Run the Application

Start the Flask development server:

```bash
python app.py
```

**Expected output:**
```
INFO:__main__:‚úÖ Flask app initialized
INFO:__main__:   Default model: openai
 * Serving Flask app 'app'
 * Debug mode: on
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.3:5000
Press CTRL+C to quit
```

**Server is running!** ‚úÖ

---

### Step 9: Access the Application

Open your browser and visit:

**http://127.0.0.1:5000**

You should see the ALS Caregiver's Compass homepage!

---

### Step 10: Test AI Assistant

1. Click **"AI Assistant"** in navigation
2. See the **model selector dropdown** at top
3. Select your model (e.g., "OpenAI GPT-4o-mini")
4. Type a question: **"What support is available in India for ALS patients?"**
5. Click **Send**
6. Get AI-powered response! üéâ

---

## üìÅ Directory Structure

```
als-caregiver-compass/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ app.py                      # Main Flask application
‚îú‚îÄ‚îÄ üìÑ ai_system_unified.py        # Multi-model AI system
‚îú‚îÄ‚îÄ üìÑ vector_store.py             # ChromaDB interface
‚îú‚îÄ‚îÄ üìÑ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ üìÑ .env                        # Environment variables (YOUR API KEYS)
‚îú‚îÄ‚îÄ üìÑ .env.example                # Environment template
‚îú‚îÄ‚îÄ üìÑ ZEROTOHERO.md              # This guide! üéâ
‚îÇ
‚îú‚îÄ‚îÄ üìä Database Scripts
‚îÇ   ‚îú‚îÄ‚îÄ clear_database.py          # Clear DB with warnings
‚îÇ   ‚îú‚îÄ‚îÄ ingest_data_fast.py        # Optimized data loading
‚îÇ   ‚îî‚îÄ‚îÄ test_models.py             # Test API keys
‚îÇ
‚îú‚îÄ‚îÄ üìÇ data/                       # Source data files
‚îÇ   ‚îú‚îÄ‚îÄ sources.yaml               # Medical sources
‚îÇ   ‚îî‚îÄ‚îÄ whatsapp_anonymized.txt    # Community chats
‚îÇ
‚îú‚îÄ‚îÄ üìÇ chroma_db/                  # Vector database (auto-created)
‚îÇ   ‚îú‚îÄ‚îÄ chroma.sqlite3
‚îÇ   ‚îî‚îÄ‚îÄ ... (embeddings)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ templates/                  # HTML templates
‚îÇ   ‚îú‚îÄ‚îÄ base.html                  # Master template
‚îÇ   ‚îú‚îÄ‚îÄ index.html                 # Homepage
‚îÇ   ‚îú‚îÄ‚îÄ ai_assistant.html          # Chat interface
‚îÇ   ‚îú‚îÄ‚îÄ emergency_protocol.html
‚îÇ   ‚îú‚îÄ‚îÄ understanding_als.html
‚îÇ   ‚îú‚îÄ‚îÄ daily_schedule.html
‚îÇ   ‚îú‚îÄ‚îÄ communication.html
‚îÇ   ‚îú‚îÄ‚îÄ home_icu_guide.html
‚îÇ   ‚îú‚îÄ‚îÄ experiences.html
‚îÇ   ‚îú‚îÄ‚îÄ faq.html
‚îÇ   ‚îú‚îÄ‚îÄ 404.html
‚îÇ   ‚îî‚îÄ‚îÄ 500.html
‚îÇ
‚îú‚îÄ‚îÄ üìÇ static/                     # Static assets
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css              # Main stylesheet
‚îÇ   ‚îú‚îÄ‚îÄ js/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.js                # Client-side JavaScript
‚îÇ   ‚îî‚îÄ‚îÄ images/
‚îÇ       ‚îî‚îÄ‚îÄ logo.png
‚îÇ
‚îî‚îÄ‚îÄ üìÇ venv/                       # Virtual environment (gitignored)
```

---

## üéÆ Running the Application

### Development Mode

```bash
# Activate virtual environment
source venv/bin/activate  # macOS/Linux
.\venv\Scripts\activate   # Windows

# Run Flask app
python app.py
```

**Features:**
- Debug mode enabled
- Auto-reload on code changes
- Detailed error pages
- Running on http://127.0.0.1:5000

### Production Mode (Future)

```bash
# Install production server
pip install gunicorn

# Run with gunicorn
gunicorn -w 4 -b 0.0.0.0:8000 app:app
```

**Features:**
- Multiple workers
- Better performance
- Process management
- Production-ready

---

## üîç Feature Deep Dive

### 1. Multi-Model AI System

**Architecture:**
```python
class UnifiedAISystem:
    def __init__(self, model_provider='openai'):
        # Initialize based on provider
        if model_provider == 'openai':
            self._init_openai()
        elif model_provider == 'gemini':
            self._init_gemini()
        elif model_provider == 'claude':
            self._init_claude()
        elif model_provider == 'grok':
            self._init_grok()
```

**How Model Selection Works:**

1. **Frontend (HTML):**
```html
<select id="modelSelect">
    <option value="openai">OpenAI GPT-4o-mini</option>
    <option value="gemini">Google Gemini 2.0 Flash</option>
    <option value="claude">Claude Sonnet 4.5</option>
    <option value="grok">Grok 2</option>
</select>
```

2. **JavaScript Sends Selection:**
```javascript
const selectedModel = document.getElementById('modelSelect').value;

fetch('/api/ai-assistant', {
    method: 'POST',
    body: JSON.stringify({
        message: userQuestion,
        model: selectedModel  // 'openai', 'gemini', etc.
    })
});
```

3. **Backend Routes to Provider:**
```python
@app.route('/api/ai-assistant', methods=['POST'])
def ai_assistant_chat():
    selected_model = request.json.get('model', 'openai')
    ai_system = UnifiedAISystem(model_provider=selected_model)
    response = ai_system.process_query(message)
    return jsonify(response)
```

**Benefits:**
- ‚úÖ Switch models without restart
- ‚úÖ Compare responses
- ‚úÖ Fallback options
- ‚úÖ Cost optimization

---

### 2. RAG (Retrieval-Augmented Generation)

**How RAG Works:**

```mermaid
graph TD
    A[User Question] --> B[Convert to Embedding]
    B --> C[Search ChromaDB]
    C --> D[Find Similar Documents]
    D --> E[Rank by Relevance + Trust]
    E --> F[Top 5 Results]
    F --> G[Format as Context]
    G --> H[Send to LLM]
    H --> I[LLM Generates Answer]
    I --> J[Add Source Citations]
    J --> K[Return to User]
    
    style A fill:#e1f5ff
    style H fill:#fff3e0
    style K fill:#e8f5e9
```

**Code Implementation:**

```python
def process_query(self, query: str):
    # 1. Detect emergency
    is_emergency = self._detect_emergency(query)
    
    # 2. Retrieve context from ChromaDB
    medical = self.vector_store.query("medical_knowledge", query, n=3)
    community = self.vector_store.query("community_experiences", query, n=2)
    
    # 3. Prepare context with prioritization
    context = self._prepare_context(medical, community)
    
    # 4. Synthesize response with LLM
    response = self._synthesize_response(query, context, is_emergency)
    
    return response
```

**Prioritization Algorithm:**

```python
def _prepare_context(self, medical, community):
    scored_docs = []
    
    for doc in medical + community:
        score = doc['distance']  # Semantic similarity
        
        # Boost India-specific sources
        if 'India' in doc['metadata'].get('source', ''):
            score += 2.0
        
        # Boost high trust sources
        trust = doc['metadata'].get('trust_score', 5)
        score += (trust / 10)
        
        scored_docs.append((score, doc))
    
    # Sort by score and return top 5
    sorted_docs = sorted(scored_docs, key=lambda x: x[0], reverse=True)
    return sorted_docs[:5]
```

---

### 3. Vector Embeddings

**What are Embeddings?**

Embeddings convert text into numerical vectors that capture semantic meaning.

**Example:**
```python
text = "What is ALS?"
embedding = [0.23, -0.45, 0.67, ..., 0.12]  # 384 numbers
```

**Similar texts have similar vectors:**
- "What is ALS?" ‚âà "Explain ALS disease"
- "What is ALS?" ‚â† "Best pizza recipe"

**Our Model:**
- **Sentence-Transformers: all-MiniLM-L6-v2**
- 384 dimensions
- Fast & lightweight
- Runs locally (no API calls)

**Usage in Code:**

```python
from sentence_transformers import SentenceTransformer

# Initialize model (once)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Create embedding
text = "Patient needs respiratory support"
embedding = model.encode(text)  # ‚Üí [384 numbers]

# Store in ChromaDB
collection.add(
    documents=[text],
    embeddings=[embedding],
    ids=["doc_001"]
)

# Query (finds similar)
results = collection.query(
    query_embeddings=[model.encode("breathing problems")],
    n_results=5
)
```

---

### 4. Database Management

**Two Collections:**

1. **medical_knowledge**
   - Sources: WHO, PubMed, ALS Care India
   - Trust score: 8-10
   - ~12 documents

2. **community_experiences**
   - Source: WhatsApp groups
   - Trust score: 7
   - ~448 messages (limited for performance)

**Clear Database (Safe):**

```bash
python clear_database.py
```

**Safety Features:**
1. Shows current data count
2. Warns about data loss
3. Requires typing `DELETE` (exact caps)
4. Asks yes/no confirmation
5. Checks if data files exist

**Ingest Database (Optimized):**

```bash
python ingest_data_fast.py
```

**Optimizations:**
1. Limits to 500 WhatsApp messages
2. Skips empty/short lines
3. Shows progress bars
4. PII scrubbing
5. Completes in ~30 seconds

---

## üîå API Reference

### Endpoints

#### 1. Homepage
```
GET /
```

**Response:** HTML page

---

#### 2. AI Assistant Page
```
GET /ai-assistant
```

**Response:** HTML page with chat interface

---

#### 3. AI Chat API
```
POST /api/ai-assistant
```

**Request Body:**
```json
{
    "message": "What support is available in India?",
    "model": "openai"
}
```

**Parameters:**
- `message` (string, required): User's question
- `model` (string, optional): AI model to use
  - Options: `openai`, `gemini`, `claude`, `grok`
  - Default: From `.env` (`DEFAULT_MODEL_PROVIDER`)

**Response (Success):**
```json
{
    "response": "Based on ALS Care and Support India...",
    "is_emergency": false,
    "timestamp": "2025-12-11T10:30:00",
    "model_used": "openai/gpt-4o-mini",
    "sources": [
        {
            "source": "ALS Care and Support India",
            "relevance": 0.89
        }
    ]
}
```

**Response (Error):**
```json
{
    "error": "Internal server error",
    "response": "I'm having trouble connecting. Error: API key not found"
}
```

**Status Codes:**
- `200`: Success
- `400`: Bad request (empty message)
- `500`: Server error (API issue)

---

#### 4. Health Check
```
GET /api/health
```

**Response:**
```json
{
    "status": "healthy",
    "ai_system": "ready",
    "openai_api": "configured",
    "gemini_api": "configured",
    "claude_api": "missing",
    "default_model": "openai",
    "version": "1.0.0"
}
```

---

## üêõ Troubleshooting

### Common Issues

#### 1. "OPENAI_API_KEY not found in .env"

**Problem:** API key missing or incorrect in `.env`

**Solution:**
1. Open `.env` file
2. Add: `OPENAI_API_KEY=sk-proj-your-key-here`
3. No spaces around `=`
4. Restart Flask app: Stop (Ctrl+C), run `python app.py`

---

#### 2. "Gemini quota exceeded"

**Problem:** Hit free tier limits (15 requests/min, 1500/day)

**Solution:**
1. Wait 1 minute and try again
2. OR switch to OpenAI:
   - Change dropdown to "OpenAI"
   - Add OpenAI key to `.env`

---

#### 3. "Claude credits too low"

**Problem:** No credits in Anthropic account

**Solution:**
1. Visit: https://console.anthropic.com/settings/billing
2. Add payment method
3. Purchase credits ($5 minimum)
4. OR use OpenAI/Gemini instead

---

#### 4. Ingestion taking forever

**Problem:** Using old `ingest_data.py` (processes all lines)

**Solution:**
1. Stop current process (Ctrl+C)
2. Use optimized version:
   ```bash
   python ingest_data_fast.py
   ```
3. Should complete in ~30 seconds

---

#### 5. "PermissionError" when clearing database

**Problem:** Database files locked (app still running)

**Solution:**
1. Stop Flask app (Ctrl+C in app terminal)
2. Run clear script:
   ```bash
   python clear_database.py
   ```

---

#### 6. Model selector not working

**Problem:** Old browser cache

**Solution:**
1. Hard refresh browser:
   - Windows: Ctrl + Shift + R
   - Mac: Cmd + Shift + R
2. Or clear browser cache

---

#### 7. ChromaDB telemetry errors (safe to ignore)

**Problem:** Harmless warnings about telemetry

**Message:**
```
ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event...
```

**Solution:**
- These are safe to ignore
- Don't affect functionality
- ChromaDB still works fine

---

## üéØ User Guide

### How to Use AI Assistant

**Step 1: Navigate**
- Click "AI Assistant" in navigation bar

**Step 2: Select Model**
- See dropdown at top: "ü§ñ AI Model"
- Choose based on needs:
  - **OpenAI**: Best balance, reliable
  - **Gemini**: Free, good quality
  - **Claude**: Best quality, expensive

**Step 3: Ask Question**
- Type in chat box
- Examples:
  - "What are early signs of ALS?"
  - "How to manage respiratory issues?"
  - "What support is available in India?"

**Step 4: Get Response**
- AI retrieves relevant information
- Synthesizes answer
- Provides India-focused guidance
- Lists sources

**Step 5: Follow Up**
- Ask clarifying questions
- Switch models to compare
- Save useful information

---

### Model Comparison

| Feature | OpenAI | Gemini | Claude | Grok |
|---------|--------|--------|--------|------|
| **Cost** | $ | FREE | $$$ | $ |
| **Speed** | Fast | Medium | Medium | Fast |
| **Quality** | Excellent | Good | Best | Good |
| **Limit** | Pay-as-go | 15/min | Credits | TBD |
| **Best For** | Production | Testing | Critical | Alt |

---

## üöÄ Future Improvements

### Planned Features

1. **Enhanced Data Ingestion**
   - Conversation-based chunking
   - Timestamp extraction
   - Speaker role identification
   - Topic classification with LLM
   - Real-time ingestion

2. **Better Embeddings**
   - Switch to medical-domain embeddings
   - Consider: PubMedBERT, BioBERT
   - Multi-lingual support (Hindi, regional)

3. **Advanced RAG**
   - Hybrid search (semantic + keyword)
   - Re-ranking with cross-encoders
   - Query expansion
   - Contextual compression

4. **User Features**
   - User authentication
   - Chat history persistence
   - Favorite responses
   - Export conversations
   - Feedback mechanism

5. **Analytics**
   - Question patterns analysis
   - Popular topics
   - Model performance comparison
   - Usage statistics

6. **Production Ready**
   - Docker deployment
   - CI/CD pipeline
   - Monitoring & logging
   - Rate limiting
   - Caching layer

7. **Content Expansion**
   - More medical sources
   - Video tutorials
   - Regional language support
   - Expert Q&A integration

---

## üìù Maintenance Guide

### Regular Tasks

**Weekly:**
- Check API usage & costs
- Review chat logs for improvements
- Update medical sources if available

**Monthly:**
- Update dependencies:
  ```bash
  pip install --upgrade -r requirements.txt
  ```
- Review model performance
- Backup database:
  ```bash
  cp -r chroma_db chroma_db_backup_$(date +%Y%m%d)
  ```

**As Needed:**
- Add new WhatsApp data
- Update emergency protocols
- Refresh India-specific information

---

## üéì Learning Resources

### Understanding RAG
- https://www.pinecone.io/learn/retrieval-augmented-generation/
- https://research.ibm.com/blog/retrieval-augmented-generation-RAG

### Vector Databases
- ChromaDB Docs: https://docs.trychroma.com/
- Embeddings Guide: https://simonwillison.net/2023/Oct/23/embeddings/

### LLM APIs
- OpenAI Docs: https://platform.openai.com/docs/
- Gemini Docs: https://ai.google.dev/docs
- Claude Docs: https://docs.anthropic.com/

### Flask
- Flask Quickstart: https://flask.palletsprojects.com/

---

## üèÜ Success Metrics

### System is Working When:

‚úÖ **Health Check:**
```bash
curl http://127.0.0.1:5000/api/health
# Returns: {"status": "healthy"}
```

‚úÖ **Test Chat:**
- Model selector visible
- Questions get responses
- Sources attributed
- No errors in console

‚úÖ **Data Ingestion:**
- Completes in <1 minute
- Shows progress bars
- Loads 12 + 448 items

‚úÖ **Database:**
- Can clear safely
- Can reload successfully
- Queries work

---

## üéâ You're a Hero!

You've completed the Zero to Hero guide! You now understand:

‚úÖ **Architecture** - How all components fit together  
‚úÖ **Tech Stack** - What each technology does  
‚úÖ **Data Flow** - From ingestion to response  
‚úÖ **Setup** - Complete installation from scratch  
‚úÖ **Usage** - How to use all features  
‚úÖ **Troubleshooting** - Fix common issues  
‚úÖ **Future** - What's next for the project

**You're ready to:**
- Deploy to production
- Customize for needs
- Add new features
- Help others set up
- Contribute improvements

---

## üìû Quick Reference

### Essential Commands

```bash
# Activate environment
source venv/bin/activate  # macOS/Linux
.\venv\Scripts\activate   # Windows

# Test API keys
python test_models.py

# Clear database
python clear_database.py

# Load data
python ingest_data_fast.py

# Run app
python app.py

# Stop app
Ctrl+C
```

### Essential Files

```
.env                    ‚Üí API keys
app.py                  ‚Üí Main application
ai_system_unified.py    ‚Üí AI logic
ingest_data_fast.py     ‚Üí Data loading
ZEROTOHERO.md          ‚Üí This guide!
```

### Essential Links

- **OpenAI Keys:** https://platform.openai.com/api-keys
- **Gemini Keys:** https://aistudio.google.com/app/apikey
- **Claude Keys:** https://console.anthropic.com/settings/keys
- **App (local):** http://127.0.0.1:5000

---

**üéä Congratulations! You're now an ALS Caregiver's Compass Expert! üéä**

---

*Last Updated: December 11, 2025*  
*Version: 1.0.0*  
*Author: ALS Caregiver's Compass Team*
